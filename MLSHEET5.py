# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A1G7fKzrNQPAi391ha_U7CwrpRns-2YT
"""

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, confusion_matrix, adjusted_rand_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn.pipeline import Pipeline
import time

iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

# Optional: convert to DataFrame for easier handling
df = pd.DataFrame(X, columns=feature_names)
df['species'] = y

# Step 2: Standardize features and apply PCA
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA (first 3 components)
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

# Explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_
print("Explained variance ratio for first 3 PCs:", explained_variance_ratio)

# Step 3: Reduce to 2 components for visualization
pca_2 = PCA(n_components=2)
X_pca_2 = pca_2.fit_transform(X_scaled)

# Scatter plot colored by species
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
for i, target_name in enumerate(target_names):
    plt.scatter(X_pca_2[y == i, 0], X_pca_2[y == i, 1], label=target_name)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA of Iris Dataset (2 components)')
plt.legend()
plt.show()

# Step 4: Short note
variance_2PCs = sum(pca_2.explained_variance_ratio_)
print(f"Total variance captured by first 2 PCs: {variance_2PCs:.2f}")
print("2D provides a good visual summary, but some variance (~10%) is still lost compared to 3D.")

# Step 1: Run KMeans for k in {2..8}
inertia_list = []
silhouette_list = []
K_range = range(2, 9)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    inertia_list.append(kmeans.inertia_)
    silhouette_list.append(silhouette_score(X_scaled, labels))
    # Step 2: Elbow and silhouette plots
plt.figure(figsize=(12,5))

# Elbow plot
plt.subplot(1,2,1)
plt.plot(K_range, inertia_list, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method')

# Silhouette plot
plt.subplot(1,2,2)
plt.plot(K_range, silhouette_list, 'ro-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis')

plt.tight_layout()
plt.show()

# Step 3: Choose optimal k (example: k=3 for Iris)
optimal_k = 3
kmeans_opt = KMeans(n_clusters=optimal_k, random_state=42)
cluster_labels = kmeans_opt.fit_predict(X_scaled)

# Step 4: Compare cluster labels with true labels
conf_matrix = confusion_matrix(y, cluster_labels)
ari_score = adjusted_rand_score(y, cluster_labels)

print("Confusion Matrix:\n", conf_matrix)
print(f"Adjusted Rand Index: {ari_score:.2f}")

# Step 5: Visualize clusters on PCA 2D plot
plt.figure(figsize=(8,6))
markers = ['o', 's', '^']  # Different shapes for true species
for i, target_name in enumerate(target_names):
    plt.scatter(
        X_pca_2[y == i, 0],
        X_pca_2[y == i, 1],
         c=cluster_labels[y == i],  # color by cluster
        cmap='viridis',
        marker=markers[i],
        label=target_name
    )
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title(f'KMeans Clusters on PCA (k={optimal_k})')
plt.legend()
plt.show()

k = optimal_k  # عادة 3 للـ Iris

# Define PCA components, but not more than number of features
max_components = X.shape[1]  # 4 features in Iris
m_values = [2, 5, 10, max_components]
m_values = [m for m in m_values if m <= max_components]  # keep only valid values

results = []

for m in m_values:
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('pca', PCA(n_components=m)),
        ('kmeans', KMeans(n_clusters=k, random_state=42))
    ])

    start_time = time.time()
    cluster_labels = pipeline.fit_predict(X)
    elapsed_time = time.time() - start_time
    inertia = pipeline.named_steps['kmeans'].inertia_

    # compute silhouette score using transformed PCA data
    X_pca_m = pipeline.named_steps['pca'].transform(pipeline.named_steps['scaler'].transform(X))
    silhouette = silhouette_score(X_pca_m, cluster_labels)

    results.append((m, inertia, silhouette, elapsed_time))
    print(f"PCA components: {m}, Inertia: {inertia:.2f}, Silhouette: {silhouette:.2f}, Time: {elapsed_time:.4f}s")

# Optional: visualize results
ms, inertias, silhouettes, times = zip(*results)

plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(ms, inertias, 'bo-')
plt.xlabel('PCA components (m)')
plt.ylabel('Inertia')
plt.title('Effect of Dimensionality on Inertia')

plt.subplot(1,2,2)
plt.plot(ms, silhouettes, 'ro-')
plt.xlabel('PCA components (m)')
plt.ylabel('Silhouette Score')
plt.title('Effect of Dimensionality on Silhouette')

plt.tight_layout()
plt.show()

wine = load_wine()
X = wine.data
y = wine.target
feature_names = wine.feature_names
target_names = wine.target_names

df = pd.DataFrame(X, columns=feature_names)
df['target'] = y

# Step 2: Standardize and apply PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA()
X_pca = pca.fit_transform(X_scaled)
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = explained_variance_ratio.cumsum()
plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_variance_ratio)+1), cumulative_variance, 'bo-')
plt.xlabel('Number of PCA components')
plt.ylabel('Cumulative explained variance')
plt.title('Explained Variance Curve')
plt.grid(True)
plt.show()
# Choose n_components ~90% variance
n_components = 5
pca_n = PCA(n_components=n_components)
X_pca_n = pca_n.fit_transform(X_scaled)

# Step 3: K-means clustering
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
cluster_labels = kmeans.fit_predict(X_pca_n)

sil_score = silhouette_score(X_pca_n, cluster_labels)
print(f"Silhouette Score: {sil_score:.2f}")
# Step 4: Visualize clusters
plt.figure(figsize=(8,6))
markers = ['o', 's', '^']
for i, target_name in enumerate(target_names):
    plt.scatter(
        X_pca_n[y == i, 0],
        X_pca_n[y == i, 1],
        c=cluster_labels[y == i],
        cmap='viridis',
        marker=markers[i],
        label=target_name
    )
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('KMeans Clusters on Wine dataset (PCA space)')
plt.legend()
plt.show()

"""Assignments"""

from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import pandas as pd

# Load dataset
wine = load_wine()
X = wine.data
y = wine.target
feature_names = wine.feature_names
target_names = wine.target_names

df = pd.DataFrame(X, columns=feature_names)
df['target'] = y

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = explained_variance_ratio.cumsum()

# Plot explained variance
plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_variance_ratio)+1), cumulative_variance, 'bo-')
plt.xlabel('Number of PCA components')
plt.ylabel('Cumulative explained variance')
plt.title('Explained Variance Curve')
plt.grid(True)
plt.show()

# Choose number of components to capture ~90% variance
n_components = 5
pca_n = PCA(n_components=n_components)
X_pca_n = pca_n.fit_transform(X_scaled)

# Choose optimal k (Wine has 3 classes)
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
cluster_labels = kmeans.fit_predict(X_pca_n)

# Evaluate clustering
sil_score = silhouette_score(X_pca_n, cluster_labels)
print(f"Silhouette Score: {sil_score:.2f}")

# Visualize first 2 PCA components
plt.figure(figsize=(8,6))
markers = ['o', 's', '^']  # marker shape by true label
for i, target_name in enumerate(target_names):
    plt.scatter(
        X_pca_n[y == i, 0],
        X_pca_n[y == i, 1],
        c=cluster_labels[y == i],
        cmap='viridis',
        marker=markers[i],
        label=target_name
    )
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('KMeans Clusters on PCA space')
plt.legend()
plt.show()